# BCM3D
#### Description

BCM3D is an image analysis workflow that combines deep learning with mathematical image analysis to accurately segment and classify single bacterial cells within biofilms in 3D fluorescence images. In BCM3D, deep convolutional neural networks (CNNs) are trained using simulated biofilm images with experimentally realistic SBRs, cell densities, labeling methods, and cell shapes.

#### Usage

Here is a brief introduction of each module.  Examples and details can be found within each module separately. All experimental data shown in the manuscript can be found in the "experimentalData" folder. Estimated running time is within a few minutes for each module unless stated otherwise.

<u>1. Simulation of 3D Biofilm images (Estimated running time for one dataset is about 40 mins)</u>

 In order to simulate realistic 3D fluorescence biofilm images, CellModeller was used in this work to model cellular arrangements within bacterial biofilms. CellModeller is a framework for the modeling of large microbial communities.  Please see detailed descriptions and installation instructions at https://haselofflab.github.io/CellModeller/. 3D fluorescence biofilm images were generated by our customized MATLAB code. Please se detailed workflow in the "biofilmSim" folder.



<u>2. Generating training data</u>

In the paper, we used a common 3-classes (5-classes for heterogenous biofilms) labeling strategies in CNNs training: label background voxels as zero, cell interiors as one and cell boundaries as two. Some example Jupyter Notebook scripts are provided. These scripts shown here can be used to generate 3D labels from simulated images for training a 3D Unet (NiftyNet platform). NiftyNet is a open source CNN platforms. For more details, please go to the official website, https://niftynet.io/Training. Configuration files of CNNs training and inference are provided.



<u>3. Data Processing</u>

In order to further process the confidence map derived from CNNs, we first did a simple thresholding, as well as small object removal and dilation. Thresholding takes CNN confidence maps and output segmentation masks. A high threshold between 0.88 and 0.94 was found to give the best segmentation results with well-trained CNN models. Secondly, we performed Lcuts to further separate undersegmented objects. Please see more details in the paper and this website:  https://github.com/jwang-c/Postprocessing-using-LCuts).



<u>4. Evaluation</u>

Segmentation masks were compared with simulated 3D Ground Truth, 2D manual annotations or 3D manual annotations when the GTs were not available. Please see details in the "evaluation" folder.



<u>5. Manual annotations</u>

Please see detailed description and code in the "manual_contour"  folder for generating manual annotations for 3D experimental data.



#### System and requirements

<u>MATLAB</u>

All MATLAB code was tested in MATLAB 2019b

following toolboxes were used:

Image Processing Toolbox                              Version 11.0        (R2019b)
Parallel Computing Toolbox                            Version 7.1         (R2019b)
Statistics and Machine Learning Toolbox               Version 11.6        (R2019b)
Symbolic Math Toolbox                                 Version 8.4         (R2019b)

All MATLAB code was run in our workstation with following configurations:

System: Windows 10 Education

Processor: Intel(R) Xeon(R) CPU E5-2630 v3 2.40GHz  2.40GHz (2 processors)

Installed memory (RAM): 64.0 GB

<u>CellModeller</u>

CellModeller was tested on macOS Mojave.

Processor: 2.6 GHz Intel Core i5

Memory: 8GB 1600 MHz DDR3

Graphics: Intel Iris 1536 MB

<u>NiftyNet</u>

NiftyNet was installed and run on Rivanna, the UVA High-Performance Computing (HPC) system, with Nvidia Tesla v100 16GB GPU. Training usually takes approximately 24 hours with a single GPU.

 <u>Jupyter Notebook scripts</u>

Example Jupyter Notebook scripts were tested with Anaconda python 3.6 on our workstation stated previously.

package dependencies:  nibabel, numpy, and scikit-image. 



#### License

This project is licensed under the terms of the MIT license.




