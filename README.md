# BCM3D#### DescriptionBCM3D is an image analysis workflow that combines deep learning with mathematical image analysis to accurately segment and classify single bacterial cells within biofilms in 3D fluorescence images. In BCM3D, deep convolutional neural networks (CNNs) are trained using simulated biofilm images with experimentally realistic SBRs, cell densities, labeling methods, and cell shapes.Citation for this image analysis workflow:DOI: 10.5281/zenodo.4088658#### UsageHere is a brief introduction of each module.  Examples and details can be found within each module separately. All experimental data shown in the manuscript can be found in the "experimentalData" folder. Estimated running time is within a few minutes for each module unless stated otherwise.**<u>1. Simulation of 3D Biofilm images (Estimated running time for one dataset is about 40 mins)</u>** In order to simulate realistic 3D fluorescence biofilm images, CellModeller was used in this work to model cellular arrangements within bacterial biofilms. CellModeller is a framework for the modeling of large microbial communities.  Please see detailed descriptions and installation instructions at https://haselofflab.github.io/CellModeller/. 3D fluorescence biofilm images were generated by our customized MATLAB code. Please se detailed workflow in the "biofilmSim" folder.In the "biofilmSim" folder:- **cellModeller** - Transfer simulation result from CellModeller to a .mat file. Codes in this folder are used for CellModeller data generation and processing.CellModeller is a Python-based framework for modelling large-scale multi-cellular systems that was developed by the Haseloff lab. We have only tested CellModeller on Mac Mojave. For more details and installation instructions of CellModeller, see the official website https://haselofflab.github.io/CellModeller/. An example python script to use CellModeller is shown in <u>Examples</u>. - CellModeller saves pickled biofilm cell arrangement data, and we used *load\_pickle.ipynb* to save both the cell positions and orientations in two separate csv files. - parameters\_simulation\_v2.m combines two csv files, 'cell\_parameters.csv' and 'orientation.csv', to a .mat file, 'cell\_parameter\_eachframe.mat' for downstream biofilm simulations. The file contains cell radius, length, positions and orientations. (example .csv files are in ./csvFiles/ecoli\_csv)**PSF\_generate** - Process PSFs of  our home-build lattice light sheet microscope to match the voxel size and axes' orientations of the simulated fluorescence images.- *LoadPSF was designed specifically for processing experimental PSF of our home-build lattice light sheet microscope.*It will read two experimentally obtained PSFs (e.g. 488psf, 488psf_decon) and process them properly to match voxel size and axial direction of simulated fluorescence images. Output will be stored in 'PSF.mat'. 'PSF.mat' includes 3 variables, 'psf\_conv' (PSF used to convolve simulated emitter to get fluorescence images ), 'psf\_decon' (PSF used to deconv fluorescence images), 'background'(background signal estimate from experimental psf).**simMixedLabels** - simulate fluorescence images of mixlabel biofilms (e.g. membrane label vs membrane&cytosol label) and generate corresponding ground truth.- Mixlabeled biofilms means multi population biofilms that cells have different labelling protocols ( e.g. membrane  vs. cytosolic&membrane labeling).- The master script is run\_cellmodeler\_convolution\_mixlabel.m .- *inputs:*- It reads CellModeller simulation results from 'cell\_parameter\_eachframe.mat' and gets psf from 'PSF488.mat', parameters can be changed accordingly.- *outputs:*- It saves .tif ground truth separately into Result/surf\_gt and Result/surfinter\_gt- It saves raw simulated fluorescence images .tif into Result/mixlabel\_raw- It saves deconvolved simulated fluorescence images .nii into Result/mixlabel\_deconv**simMixedShapes** - simulate fluorescence images of mixshape biofilms (e.g. rod-shaped vs spherical cells ) and generate corresponding ground truth.- Mixshaped biofilms means multi population biofilms that cells have different shapes (e.g. spherical vs. rod shaped cells). - The master script is run\_cellmodeler\_convolution\_mixshape.m .- *inputs:*- It reads CellModeller simulation results from 'cell\_parameter\_eachframe.mat' and gets psf from 'PSF488.mat', parameters can be changed accordingly.- Here,  CellModeller only simulates rod shape cells, spherical shape cells are generated by shortening randomly selected rod shape cells in long axis. - *outputs:*- It saves .tif ground truth separately into Result/rod_gt and Result/sphere\_gt- It saves raw simulated fluorescence stack .tif into Result/mixshape\_raw- It saves deconvolved simulated fluorescence stack .tif into Result/mixshape\_deconv**simMultiDensityandSBRs** - simulate various SBR fluorescence images of single population biofilms with different cell density and generate corresponding ground truth.- The master script is biofilmSim_loadParameter.m .- *inputs:*- It reads csv parameter from configuration file."parameter\_config.csv".- It uses a main class and class function: CellModeller\_Convolution\_final.m.- Choose experimentally measured LLSM PSF between 561psf and 488psf, both raw images and deconvolved images. - *outputs:*- It saves membrane labeled cells biofilm as 'membrane'  .tif ground truth and .nii simulated fluorescence images into ./result/membrane/<u>date</u> and its subfolders; - It saves cytosolic labeled cells biofilm as 'wholeexp' labeled tif ground truth and nii simulated  fluorescence images into ./result/wholeexp/<u>date</u> and its subfolders.**<u>2. Generating training data</u>**In the paper, we used a common 3-classes (5-classes for heterogenous biofilms) labeling strategies in CNNs training: label background voxels as zero, cell interiors as one and cell boundaries as two. Some example Jupyter Notebook scripts are provided. These scripts shown here can be used to generate 3D labels from simulated images for training a 3D Unet (NiftyNet platform). NiftyNet is a open source CNN platforms. For more details, please go to the official website, https://niftynet.io/Training. Configuration files of CNNs training and inference are provided.In the "trainingDataGenerate" folder:- To generate training data for CNNs- We used a common 3-classes (5-classes for heterogenous biofilms) labeling strategies in our CNNs training: label background voxels as zero, cell interiors as one and cell boundaries as two.- These scripts shown here can be used to generate 3D labels for training (NiftyNet platform). NiftyNet is a open source CNN platforms. For more details, please go to the official website, https://niftynet.io/Training. - Jupyter Notebook scripts:- package dependencies:  nibabel,numpy, scikit-image- All the inputs should be 3D .tif ground truth images that have each cell labeled as unique integers.trainingDataGenerate generates labels for single-population biofilms(label background voxels as zero, cell interiors as one and cell boundaries as two.), whereas trainingDataGenerate\_mixLabel and trainingDataGenerate\_mixShape work for two-populations respectively (label background voxels as zero, cell interiors of population 1 as one, cell boundaries of population 1 as two, cell interiors of population 2 as three and cell boundaries of population 2 as four).- trainingDataGenerate\_mixLabel: input data should have GT pairs that have prefixes of "surf\_\_" and "surfInterior\_".- trainingDataGenerate\_mixShape: input data should have GT pairs that have suffix of "rod\_Label" and "sphere\_Label".- Examples of pretrained models are stored in folder 'pretrainedModels', each model folder contains 5 subfolders, 'config_file' stores parameters to train the network and inference data by the trained network, 'trainingdata' stores training pairs to train the network,  'testdata' stores example data for testing, 'output' stores output of the network for the test data. '...\_model' stores the network.**<u>3. Data Processing</u>**In order to further process the confidence map derived from CNNs, we first did a simple thresholding, as well as small object removal and dilation. Thresholding takes CNN confidence maps and output segmentation masks. A high threshold between 0.88 and 0.94 was found to give the best segmentation results with well-trained CNN models. Secondly, we performed Lcuts to further separate undersegmented objects. Please see more details in the paper and this website:  https://github.com/jwang-c/Postprocessing-using-LCuts).In the "dataProcessing/cell\_reconstruction" folder:- Run batch_process.m to reconstruct cells by building convex hull and then applying this the convex hull as mask to select volume for cells from the CNN results.- Input: Lcuts results- Output:- post_post_segments, contains voxels for each single cell.- post_post_seg_mat, save the whole processed data as a 3D matrix.- This step will call the function to check if points are including by a convex hull. The code is referred from the following link. John D'Errico (2020). Inhull (https://www.mathworks.com/matlabcentral/fileexchange/10226-inhull), MATLAB Central File Exchange. Retrieved May 22, 2020.In the "dataProcessing/thresholding" folder:- "Thresholding" takes CNN confidence maps and output segmentation masks. We found a high threshold between 0.88 and 0.94 yield the best segmentation results with well-trained CNN models. Since cell boundaries are eroded in the CNN processing, we also prefer to dilate each objects  by two voxels. - inputs: CNN confidence maps (.nii), threshold values (default is 0.94 for cytosol-labeled cells and 0.88 for membrane-labeled cells).- outputs: segmentation masks.**<u>4. Evaluation</u>**Segmentation masks were compared with simulated 3D Ground Truth, 2D manual annotations or 3D manual annotations when the GTs were not available. Please see details in the "evaluation" folder.In the "evaluation" folder: Codes to calculate evaluation metrics.- (1) calculate Jacard Index and cell counting accuracy - "evaluation.m" also calculates Dice, False positive percentage,   False negative percentage, Distance of major axis, and relative angle between   major axis.    When each cell  matches with  the GT cell, it  gets deleted from the list. There   can't be cells that are matched twice.    *inputs*: have to be a  directory that contains tif ground truth images and a directory that contains .tif segmentation masks from Unet or LCuts.- (2) calculate local density based on tiling of the image.- "calc\_local\_density\_v3\_tiles.m"   Tile GT images/3D mannual annotations and calculate local density:   local density = cell volume / tile volume.    *two output metrics*: maximum local density and the mean of top 10 local densities.**<u>5. Manual annotations</u>**Please see detailed description and code in the "manual_contour"  folder for generating manual annotations for 3D experimental data.- <u>(1) Run manual\_select\_contour.m to manually trace cell outlines.</u>- Select the 3D image to segment ('myxoFirstFrame\_T1.tif' is an example data.).- Then, a GUI will pop up to set up parameters for tracing cell outlines. 'x\_skip' means the gap between 2D slice of the 3D image in x dimension for tracing cell contours.- 'x0' means the start 2D slice in x dimension. 'x\_end' means the end 2D slice in x dimension. If 'x\_end' is 0, this dimension will be ignored when tracing cell contours.- 'y\_skip', 'y0', 'y\_end', 'z\_skip', 'z0' and 'z\_end' are defined in the same way.- The output of this step are three files: x\_frames, y\_frames and z\_frames. They contains all manually traced cell outlines.- <u>(2) Manually load three files from step 1: x\_frames, y\_frames and z\_frames, then run assign\_contour2cells\_v2.m to group 2D cell contours to single cells</u>- This step will call the function to extract clusters from adjecent matrix. The code is referred from the following link. Raphaël Candelier (2020). Adjacency matrix to clusters (https://www.mathworks.com/matlabcentral/fileexchange/60676-adjacency-matrix-to-clusters), MATLAB Central File Exchange. Retrieved May 22, 2020.- The output of this step is 'adjacent_results.mat' which contains all grouped contours.Run "convertCell2Mat.m" to save each contour group separately.Run "plotCheckGroupedContours.m" to check each contour group.- <u>(3) For bad contour groups, run Clustering_1st.m to manually segment cells from them.</u>This step will call the function DBSCAN.m to group points. The code is referred from the following link.Yarpiz (2020). DBSCAN Clustering Algorithm (https://www.mathworks.com/matlabcentral/fileexchange/52905-dbscan-clustering-algorithm), MATLAB Central File Exchange. Retrieved May 22, 2020.- <u>(4)For each segmented cell, run removeBadData.m to check and remove bad contours.</u>- <u>(5)Run combineDataSets.m to combine all segmented cells to one data set. </u>- ‘GT\_myxo\_firstFrame\_nofitting\_20200306.tif’ is the final annotated results.#### System and requirements<u>MATLAB</u>All MATLAB code was tested in MATLAB 2019bfollowing toolboxes were used:Image Processing Toolbox                              Version 11.0        (R2019b)Parallel Computing Toolbox                            Version 7.1         (R2019b)Statistics and Machine Learning Toolbox               Version 11.6        (R2019b)Symbolic Math Toolbox                                 Version 8.4         (R2019b)All MATLAB code was run in our workstation with following configurations:System: Windows 10 EducationProcessor: Intel(R) Xeon(R) CPU E5-2630 v3 2.40GHz  2.40GHz (2 processors)Installed memory (RAM): 64.0 GB<u>CellModeller</u>CellModeller was tested on macOS Mojave.Processor: 2.6 GHz Intel Core i5Memory: 8GB 1600 MHz DDR3Graphics: Intel Iris 1536 MB<u>NiftyNet</u>NiftyNet was installed and run on Rivanna, the UVA High-Performance Computing (HPC) system, with Nvidia Tesla v100 16GB GPU. Training usually takes approximately 24 hours with a single GPU. <u>Jupyter Notebook scripts</u>Example Jupyter Notebook scripts were tested with Anaconda python 3.6 on our workstation stated previously.package dependencies:  nibabel, numpy, and scikit-image. #### LicenseThis project is licensed under the terms of the MIT license.